From b654ff2888c3dd84760d5af81d58127f86764e34 Mon Sep 17 00:00:00 2001
From: Abderrahmane Benbachir <abderrahmane.benbachir@polymtl.ca>
Date: Thu, 12 Oct 2017 19:03:24 -0400
Subject: [PATCH] ftrace: support very early function tracing

"ftrace_vearly" is the kernel cmdline option to enable very early
function tracing. Filtering is possible using the pre-existing ftrace
features: ftrace_filter and ftrace_notrace.

Currently, the events are saved in a static array, things can be very
difficult when tracing super early, we kept the static buffer for now.

The timestamp is an other issue that was solved using rdtsc only for x86
arch, when tsc feature is not supported we use sched_clock.

Signed-off-by: Abderrahmane Benbachir <abderrahmane.benbachir@polymtl.ca>
---
 arch/x86/kernel/ftrace_32.S |  23 ++++
 arch/x86/kernel/ftrace_64.S |  10 ++
 include/linux/ftrace.h      |  13 +-
 init/main.c                 |   1 +
 kernel/trace/ftrace.c       | 311 ++++++++++++++++++++++++++++++++++++++++++++
 kernel/trace/trace.c        |  31 +++++
 kernel/trace/trace.h        |   4 +
 7 files changed, 392 insertions(+), 1 deletion(-)

diff --git a/arch/x86/kernel/ftrace_32.S b/arch/x86/kernel/ftrace_32.S
index 722a145b4139..5ef2a798c685 100644
--- a/arch/x86/kernel/ftrace_32.S
+++ b/arch/x86/kernel/ftrace_32.S
@@ -30,7 +30,30 @@ EXPORT_SYMBOL(mcount)
 #endif
 
 ENTRY(function_hook)
+	cmpl	$__PAGE_OFFSET, %esp
+	jb	vearly_stub			/* Paging not enabled yet? */
+
+	cmpl	$ftrace_stub, ftrace_vearly_trace_function
+	jnz vearly_trace
+
+vearly_stub:
 	ret
+
+vearly_trace:
+	pushl	%eax
+	pushl	%ecx
+	pushl	%edx
+	movl	0xc(%esp), %eax
+	movl	0x4(%ebp), %edx
+	subl	$MCOUNT_INSN_SIZE, %eax
+
+	call	*ftrace_vearly_trace_function
+
+	popl	%edx
+	popl	%ecx
+	popl	%eax
+
+	jmp vearly_stub
 END(function_hook)
 
 ENTRY(ftrace_caller)
diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S
index 1dfac634bbf7..8f30923ee013 100644
--- a/arch/x86/kernel/ftrace_64.S
+++ b/arch/x86/kernel/ftrace_64.S
@@ -146,7 +146,17 @@ EXPORT_SYMBOL(mcount)
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 ENTRY(function_hook)
+	cmpq $ftrace_stub, ftrace_vearly_trace_function
+	jnz vearly_trace
+
+vearly_stub:
 	retq
+
+vearly_trace:
+	save_mcount_regs
+	call *ftrace_vearly_trace_function
+	restore_mcount_regs
+	jmp vearly_stub
 END(function_hook)
 
 ENTRY(ftrace_caller)
diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2e028854bac7..baf80962980b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -256,6 +256,10 @@ static inline int ftrace_function_local_disabled(struct ftrace_ops *ops)
 extern void ftrace_stub(unsigned long a0, unsigned long a1,
 			struct ftrace_ops *op, struct pt_regs *regs);
 
+extern void ftrace_early_init(char *command_line);
+extern void ftrace_early_shutdown(void);
+extern void ftrace_early_fill_ringbuffer(void *data);
+
 #else /* !CONFIG_FUNCTION_TRACER */
 /*
  * (un)register_ftrace_function must be a macro since the ops parameter
@@ -270,6 +274,9 @@ static inline int ftrace_nr_registered_ops(void)
 static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
 static inline void ftrace_free_init_mem(void) { }
+static inline void ftrace_early_init(char *command_line) { }
+static inline void ftrace_early_shutdown(void) { }
+static inline void ftrace_early_fill_ringbuffer(void *data) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_STACK_TRACER
@@ -476,6 +483,10 @@ unsigned long ftrace_get_addr_curr(struct dyn_ftrace *rec);
 
 extern ftrace_func_t ftrace_trace_function;
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+extern ftrace_func_t ftrace_vearly_trace_function;
+#endif
+
 int ftrace_regex_open(struct ftrace_ops *ops, int flag,
 		  struct inode *inode, struct file *file);
 ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
@@ -757,7 +768,7 @@ static inline unsigned long get_lock_parent_ip(void)
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
 #else
-static inline void ftrace_init(void) { }
+static inline void ftrace_init(void) { ftrace_early_shutdown(); }
 #endif
 
 /*
diff --git a/init/main.c b/init/main.c
index 0ee9c6866ada..c794292b18ef 100644
--- a/init/main.c
+++ b/init/main.c
@@ -511,6 +511,7 @@ asmlinkage __visible void __init start_kernel(void)
 	char *command_line;
 	char *after_dashes;
 
+	ftrace_early_init(boot_command_line);
 	set_task_stack_end_magic(&init_task);
 	smp_setup_processor_id();
 	debug_objects_early_init();
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index 8319e09e15b9..b4b177f25535 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -5917,6 +5917,8 @@ void __init ftrace_init(void)
 	unsigned long count, flags;
 	int ret;
 
+	ftrace_early_shutdown();
+
 	local_irq_save(flags);
 	ret = ftrace_dyn_arch_init();
 	local_irq_restore(flags);
@@ -6882,3 +6884,312 @@ void ftrace_graph_exit_task(struct task_struct *t)
 	kfree(ret_stack);
 }
 #endif
+
+/*
+ * -------------- Very Early Function Tracing ----------------
+ */
+
+#define VEARLY_BUFFER_SIZE 100
+/*
+ * In very early stage :
+ *  - only CPU0 is running in early, there is no need to store it
+ *  - tsc counter is used in x86, otherwise we use sched_clock
+ */
+struct ftrace_vearly_entry {
+	unsigned long ip;
+	unsigned long parent_ip;
+	unsigned long long clock;
+	unsigned long long cycles;
+};
+static unsigned int ftrace_vearly_enabled __initdata;
+static unsigned int vearly_entries_count __initdata;
+static struct
+	ftrace_vearly_entry ftrace_vearly_entries[VEARLY_BUFFER_SIZE] __initdata;
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+ftrace_func_t ftrace_vearly_trace_function __read_mostly = ftrace_stub;
+#else
+# define ftrace_vearly_trace_function ftrace_trace_function
+#endif
+
+#define ftrace_vearly_disable() (ftrace_vearly_trace_function = ftrace_stub)
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+
+#define VEARLY_FILTER_SIZE 10000
+static int vearly_notrace_count __initdata;
+static unsigned long ftrace_vearly_notrace_list[VEARLY_FILTER_SIZE] __initdata;
+static int vearly_filter_count __initdata;
+static unsigned long ftrace_vearly_filter_list[VEARLY_FILTER_SIZE] __initdata;
+static char ftrace_vearly_notrace_buf[COMMAND_LINE_SIZE] __initdata;
+static char ftrace_vearly_filter_buf[COMMAND_LINE_SIZE] __initdata;
+
+#define ftrace_vearly_notrace_addr(addr) bsearch(&addr, \
+ftrace_vearly_notrace_list, vearly_notrace_count, sizeof(unsigned long), \
+ftrace_addr_compare)
+#define ftrace_vearly_filter_addr(addr) bsearch(&addr, \
+ftrace_vearly_filter_list, vearly_filter_count, sizeof(unsigned long), \
+ftrace_addr_compare)
+
+static int
+ftrace_vearly_match_record(unsigned long ip, struct ftrace_glob *func_g)
+{
+	char str[KSYM_SYMBOL_LEN];
+	char *modname;
+
+	kallsyms_lookup(ip, NULL, NULL, &modname, str);
+	return ftrace_match(str, func_g);
+}
+
+static __init int ftrace_vearly_filter_has_addr(unsigned long addr,
+	unsigned long *filter, int *counter)
+{
+	int i;
+
+	for (i = 0; i < *counter; i++) {
+		if (filter[i] == addr)
+			return 1;
+	}
+	return 0;
+}
+
+static __init void
+ftrace_vearly_regex(char *func, int len, unsigned long *filter, int *counter)
+{
+	extern unsigned long __start_mcount_loc[];
+	extern unsigned long __stop_mcount_loc[];
+
+	unsigned long *start = __start_mcount_loc;
+	unsigned long *end = __stop_mcount_loc;
+	unsigned long count;
+	unsigned long addr;
+	unsigned long *p;
+	struct ftrace_glob func_g = { .type = MATCH_FULL };
+	int clear_filter = 0;
+
+	count = end - start;
+
+	if (!count)
+		return;
+
+	if (func) {
+		func_g.type = filter_parse_regex(func, len, &func_g.search,
+						 &clear_filter);
+		func_g.len = strlen(func_g.search);
+	}
+
+	p = start;
+	while (p < end) {
+		addr = ftrace_call_adjust(*p++);
+		if (!addr)
+			continue;
+
+		if ((*counter) > VEARLY_FILTER_SIZE)
+			return;
+
+		if (ftrace_vearly_match_record(addr, &func_g)) {
+			if (!ftrace_vearly_filter_has_addr(addr, filter, counter))
+				filter[(*counter)++] = addr;
+		}
+	}
+}
+
+static int ftrace_addr_compare(const void *a, const void *b)
+{
+	if (*(unsigned long *)a > *(unsigned long *)b)
+		return 1;
+	if (*(unsigned long *)a < *(unsigned long *)b)
+		return -1;
+
+	return 0;
+}
+
+static void ftrace_addr_swap(void *a, void *b, int size)
+{
+	unsigned long t = *(unsigned long *)a;
+	*(unsigned long *)a = *(unsigned long *)b;
+	*(unsigned long *)b = t;
+}
+
+void __init
+ftrace_set_vearly_filter(char *buf, unsigned long *filter, int *counter)
+{
+	char *func;
+
+	while (buf) {
+		func = strsep(&buf, ",");
+		ftrace_vearly_regex(func, strlen(func), filter, counter);
+	}
+}
+
+static __init int set_ftrace_vearly_notrace(char *str)
+{
+	strlcpy(ftrace_vearly_notrace_buf, str, COMMAND_LINE_SIZE);
+	ftrace_set_vearly_filter(ftrace_vearly_notrace_buf,
+			ftrace_vearly_notrace_list, &vearly_notrace_count);
+	/* sort filter to use bsearch on it */
+	sort(ftrace_vearly_notrace_list, vearly_notrace_count,
+		sizeof(unsigned long), ftrace_addr_compare, ftrace_addr_swap);
+
+	return 1;
+}
+
+static __init int set_ftrace_vearly_filter(char *str)
+{
+	strlcpy(ftrace_vearly_filter_buf, str, COMMAND_LINE_SIZE);
+	ftrace_set_vearly_filter(ftrace_vearly_filter_buf,
+			ftrace_vearly_filter_list, &vearly_filter_count);
+	/* sort filter to use bsearch on it */
+	sort(ftrace_vearly_filter_list, vearly_filter_count,
+		sizeof(unsigned long), ftrace_addr_compare, ftrace_addr_swap);
+
+	return 1;
+}
+
+#endif /* CONFIG_DYNAMIC_FTRACE */
+
+static __init int set_ftrace_vearly_enable(char *str)
+{
+	ftrace_vearly_enabled = 1;
+	return 1;
+}
+
+static __init void
+ftrace_function_vearly_trace_call(unsigned long ip, unsigned long parent_ip,
+			struct ftrace_ops *op, struct pt_regs *regs)
+{
+	struct ftrace_vearly_entry *entry;
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+	if (vearly_notrace_count && ftrace_vearly_notrace_addr(ip))
+		return;
+
+	if (vearly_filter_count && !ftrace_vearly_filter_addr(ip))
+		return;
+#endif
+
+	if (vearly_entries_count >= VEARLY_BUFFER_SIZE) {
+		ftrace_vearly_disable();
+		return;
+	}
+
+	entry = &ftrace_vearly_entries[vearly_entries_count++];
+
+	entry->ip = ip;
+	entry->parent_ip = parent_ip;
+#ifdef CONFIG_X86_TSC
+	entry->cycles = rdtsc();
+#else
+	entry->clock = trace_clock_local();
+#endif
+}
+
+/*
+ * ns = cycles * (10^6 / cpu_khz)
+ * formula taken from arch/x86/kernel/tsc.c
+ */
+static u64 __init cycles_to_ns(u64 cycles, u64 khz)
+{
+	u64 ns = cycles;
+	u64 dividend, divisor = 0;
+	const u64 constant = 1000000;
+
+	if (khz > constant) {
+		// ns /= (khz / constant);
+		dividend = khz;
+		divisor = constant;
+		do_div(dividend, divisor);
+		do_div(ns, dividend);
+	} else {
+		dividend = constant;
+		divisor = khz;
+		do_div(dividend, divisor);
+		ns *= dividend;
+	}
+	return ns;
+}
+void __init ftrace_early_fill_ringbuffer(void *data)
+{
+	int i;
+	u64 ns, delta, prev_ns = 0;
+	unsigned long current_cpu_khz;
+	struct ftrace_vearly_entry *entry;
+	struct trace_array *tr = data;
+
+	if (!ftrace_vearly_enabled)
+		return;
+
+	preempt_disable_notrace();
+	current_cpu_khz = native_calibrate_cpu();
+	for (i = 0; i < vearly_entries_count; i++) {
+		entry = &ftrace_vearly_entries[i];
+
+		if (entry->cycles && current_cpu_khz)
+			ns = cycles_to_ns(entry->cycles, current_cpu_khz);
+		else
+			ns = entry->clock;
+
+		delta = ns - prev_ns;
+		prev_ns = ns;
+		trace_function_timestamp(tr, entry->ip, entry->parent_ip, 0, 0, delta);
+	}
+	preempt_enable_notrace();
+}
+/*
+ * this will be used as __setup_param
+ */
+static struct obs_kernel_param ftrace_vearly_params[] __initdata = {
+	{ .str = "ftrace_vearly", .setup_func = set_ftrace_vearly_enable },
+#ifdef CONFIG_DYNAMIC_FTRACE
+	{ .str = "ftrace_notrace", .setup_func = set_ftrace_vearly_notrace },
+	{ .str = "ftrace_filter", .setup_func = set_ftrace_vearly_filter },
+#endif
+};
+
+static int __init ftrace_do_very_early_param(char *param, char *val,
+				 const char *unused, void *arg)
+{
+	int i;
+	struct obs_kernel_param *p;
+	int size = sizeof(ftrace_vearly_params)/sizeof(struct obs_kernel_param);
+
+	for (i = 0; i < size; i++) {
+		p = &ftrace_vearly_params[i];
+		if (strcmp(param, p->str) == 0) {
+			p->setup_func(val);
+			return 0;
+		}
+	}
+	return 0;
+}
+
+void __init ftrace_early_init(char *command_line)
+{
+	static char tmp_cmdline[COMMAND_LINE_SIZE] __initdata;
+
+	/* proceed only if 'vearly' exist */
+	if (!strstr(command_line, "vearly"))
+		return;
+
+	strlcpy(tmp_cmdline, command_line, COMMAND_LINE_SIZE);
+	parse_args("ftrace vearly options", tmp_cmdline, NULL, 0, 0, 0, NULL,
+		ftrace_do_very_early_param);
+
+	if (ftrace_vearly_enabled)
+		ftrace_vearly_trace_function = ftrace_function_vearly_trace_call;
+}
+
+void __init ftrace_early_shutdown(void)
+{
+	if (!ftrace_vearly_enabled)
+		return;
+
+	ftrace_vearly_disable();
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+	pr_info("ftrace: vearly %u entries, notrace=%d, filter=%d",
+	vearly_entries_count, vearly_notrace_count, vearly_filter_count);
+#else
+	pr_info("ftrace: vearly %u recorded entries", vearly_entries_count);
+#endif
+}
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 752e5daf0896..0d71d440a0de 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -2554,6 +2554,33 @@ trace_function(struct trace_array *tr,
 	}
 }
 
+void
+trace_function_timestamp(struct trace_array *tr,
+		unsigned long ip, unsigned long parent_ip, unsigned long flags,
+		int pc, u64 timestamp)
+{
+	struct trace_event_call *call = &event_function;
+	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer_event *event;
+	struct ftrace_entry *entry;
+
+	event = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),
+						flags, pc);
+	if (!event)
+		return;
+
+	event->time_delta = timestamp;
+
+	entry = ring_buffer_event_data(event);
+	entry->ip = ip;
+	entry->parent_ip = parent_ip;
+
+	if (!call_filter_check_discard(call, entry, buffer, event)) {
+		if (static_branch_unlikely(&ftrace_exports_enabled))
+			ftrace_exports(event);
+		__buffer_unlock_commit(buffer, event);
+	}
+}
 #ifdef CONFIG_STACKTRACE
 
 #define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))
@@ -8276,6 +8303,7 @@ void ftrace_dump(enum ftrace_dump_mode oops_dump_mode)
 }
 EXPORT_SYMBOL_GPL(ftrace_dump);
 
+
 __init static int tracer_alloc_buffers(void)
 {
 	int ring_buf_size;
@@ -8364,6 +8392,9 @@ __init static int tracer_alloc_buffers(void)
 	/* Function tracing may start here (via kernel command line) */
 	init_function_trace();
 
+	/* Fill ringbuffer with very early events */
+	ftrace_early_fill_ringbuffer(&global_trace);
+
 	/* All seems OK, enable tracing */
 	tracing_disabled = 0;
 
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 652c682707cd..9aa22e261812 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -624,6 +624,10 @@ void trace_init_global_iter(struct trace_iterator *iter);
 
 void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
+void trace_function_timestamp(struct trace_array *tr,
+		    unsigned long ip,
+		    unsigned long parent_ip,
+		    unsigned long flags, int pc, u64 timestamp);
 void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-- 
2.11.0

