From 43107b317cd8e2cd0e75b003a055f4767d2a84d2 Mon Sep 17 00:00:00 2001
From: Abderrahmane Benbachir <abderrahmane.benbachir@polymtl.ca>
Date: Thu, 12 Oct 2017 19:03:24 -0400
Subject: [PATCH] ftrace: support very early function tracing

"ftrace_vearly" is the kernel cmdline option the enable very early
function tracing. Filtering is possible using the pre-existing ftrace
features: ftrace_filter and ftrace_notrace.

Currently, the events are saved in a static array, bootmem can be
used, but thing can be very difficult when tracing super early, we
kept the static buffer for now.

The timestamp is an other issue that we solved with rdtsc in x86
arch, but when tsc feature is not supported we fallback to sched_clock
which only works after a certain period from the early bootup.

Signed-off-by: Abderrahmane Benbachir <abderrahmane.benbachir@polymtl.ca>
---
 arch/x86/kernel/ftrace_32.S |  47 +++++++
 arch/x86/kernel/ftrace_64.S |  20 +++
 include/linux/ftrace.h      |  10 +-
 init/main.c                 |   2 +
 kernel/trace/ftrace.c       | 308 +++++++++++++++++++++++++++++++++++++++++++-
 kernel/trace/trace.c        |  31 +++++
 kernel/trace/trace.h        |   4 +
 7 files changed, 420 insertions(+), 2 deletions(-)

diff --git a/arch/x86/kernel/ftrace_32.S b/arch/x86/kernel/ftrace_32.S
index 722a145b4139..7a1806ba4bd0 100644
--- a/arch/x86/kernel/ftrace_32.S
+++ b/arch/x86/kernel/ftrace_32.S
@@ -16,6 +16,21 @@ EXPORT_SYMBOL(__fentry__)
 EXPORT_SYMBOL(mcount)
 #endif
 
+.macro save_mcount_regs
+	pushl	%eax
+	pushl	%ecx
+	pushl	%edx
+	movl	0xc(%esp), %eax
+	movl	0x4(%ebp), %edx
+	subl	$MCOUNT_INSN_SIZE, %eax
+	.endm
+
+.macro restore_mcount_regs
+	popl	%edx
+	popl	%ecx
+	popl	%eax
+	.endm
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 /* mcount uses a frame pointer even if CONFIG_FRAME_POINTER is not set */
@@ -30,6 +45,19 @@ EXPORT_SYMBOL(mcount)
 #endif
 
 ENTRY(function_hook)
+	cmpl	$__PAGE_OFFSET, %esp
+	jb	vearly_stub			/* Paging not enabled yet? */
+
+	cmpl	$ftrace_stub, ftrace_vearly_trace_function
+	jnz vearly_trace
+	ret
+
+vearly_trace:
+	save_mcount_regs
+	call	*ftrace_vearly_trace_function
+	restore_mcount_regs
+
+vearly_stub:
 	ret
 END(function_hook)
 
@@ -176,6 +204,10 @@ ENTRY(function_hook)
 
 	cmpl	$ftrace_stub, ftrace_trace_function
 	jnz	.Ltrace
+
+	cmpl	$ftrace_stub, ftrace_vearly_trace_function
+	jnz	vearly_trace
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	cmpl	$ftrace_stub, ftrace_graph_return
 	jnz	ftrace_graph_caller
@@ -202,6 +234,21 @@ ftrace_stub:
 	popl	%ecx
 	popl	%eax
 	jmp	ftrace_stub
+
+vearly_trace:
+	pushl	%eax
+	pushl	%ecx
+	pushl	%edx
+	movl	0xc(%esp), %eax
+	movl	0x4(%ebp), %edx
+	subl	$MCOUNT_INSN_SIZE, %eax
+
+	call	*ftrace_vearly_trace_function
+	
+	popl	%edx
+	popl	%ecx
+	popl	%eax
+	jmp	ftrace_stub
 END(function_hook)
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S
index 1dfac634bbf7..da2677345b46 100644
--- a/arch/x86/kernel/ftrace_64.S
+++ b/arch/x86/kernel/ftrace_64.S
@@ -145,7 +145,17 @@ EXPORT_SYMBOL(mcount)
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 
+
 ENTRY(function_hook)
+	cmpq $ftrace_stub, ftrace_vearly_trace_function
+	jnz vearly_trace
+	retq
+
+vearly_trace:
+	save_mcount_regs
+	call *ftrace_vearly_trace_function
+	restore_mcount_regs
+
 	retq
 END(function_hook)
 
@@ -263,6 +273,9 @@ ENTRY(function_hook)
 	cmpq $ftrace_stub, ftrace_trace_function
 	jnz trace
 
+	cmpq $ftrace_stub, ftrace_vearly_trace_function
+	jnz vearly_trace
+
 fgraph_trace:
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	cmpq $ftrace_stub, ftrace_graph_return
@@ -290,6 +303,13 @@ trace:
 	restore_mcount_regs
 
 	jmp fgraph_trace
+
+vearly_trace:
+	save_mcount_regs
+	call *ftrace_vearly_trace_function
+	restore_mcount_regs
+	
+	retq
 END(function_hook)
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2e028854bac7..092321d2a4a9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -43,9 +43,15 @@
 #ifdef CONFIG_TRACING
 void trace_init(void);
 void early_trace_init(void);
+extern void ftrace_early_init(char *command_line);
+extern void ftrace_early_shutdown(void);
+extern void ftrace_early_fill_ringbuffer(void *data);
 #else
 static inline void trace_init(void) { }
 static inline void early_trace_init(void) { }
+static inline void ftrace_early_init(char *command_line) { }
+static inline void ftrace_early_shutdown(void) { }
+static inline void ftrace_early_fill_ringbuffer(void *data) { }
 #endif
 
 struct module;
@@ -476,6 +482,8 @@ unsigned long ftrace_get_addr_curr(struct dyn_ftrace *rec);
 
 extern ftrace_func_t ftrace_trace_function;
 
+extern ftrace_func_t ftrace_vearly_trace_function;
+
 int ftrace_regex_open(struct ftrace_ops *ops, int flag,
 		  struct inode *inode, struct file *file);
 ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
@@ -757,7 +765,7 @@ static inline unsigned long get_lock_parent_ip(void)
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
 #else
-static inline void ftrace_init(void) { }
+static inline void ftrace_init(void) { ftrace_early_shutdown(); }
 #endif
 
 /*
diff --git a/init/main.c b/init/main.c
index 0ee9c6866ada..b87369e0b0cc 100644
--- a/init/main.c
+++ b/init/main.c
@@ -511,6 +511,7 @@ asmlinkage __visible void __init start_kernel(void)
 	char *command_line;
 	char *after_dashes;
 
+	ftrace_early_init(boot_command_line);
 	set_task_stack_end_magic(&init_task);
 	smp_setup_processor_id();
 	debug_objects_early_init();
@@ -528,6 +529,7 @@ asmlinkage __visible void __init start_kernel(void)
 	page_address_init();
 	pr_notice("%s", linux_banner);
 	setup_arch(&command_line);
+	
 	/*
 	 * Set up the the initial canary and entropy after arch
 	 * and after adding latent and command line entropy.
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index 8319e09e15b9..f9a0d35c07b4 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -115,6 +115,7 @@ static DEFINE_MUTEX(ftrace_lock);
 
 static struct ftrace_ops __rcu *ftrace_ops_list __read_mostly = &ftrace_list_end;
 ftrace_func_t ftrace_trace_function __read_mostly = ftrace_stub;
+ftrace_func_t ftrace_vearly_trace_function __read_mostly = ftrace_stub;
 static struct ftrace_ops global_ops;
 
 #if ARCH_SUPPORTS_FTRACE_OPS
@@ -5917,6 +5918,8 @@ void __init ftrace_init(void)
 	unsigned long count, flags;
 	int ret;
 
+	ftrace_early_shutdown();
+
 	local_irq_save(flags);
 	ret = ftrace_dyn_arch_init();
 	local_irq_restore(flags);
@@ -5939,7 +5942,6 @@ void __init ftrace_init(void)
 				  __stop_mcount_loc);
 
 	set_ftrace_early_filters();
-
 	return;
  failed:
 	ftrace_disabled = 1;
@@ -6882,3 +6884,307 @@ void ftrace_graph_exit_task(struct task_struct *t)
 	kfree(ret_stack);
 }
 #endif
+
+/*
+ * ------------------ Very Early Function Tracing --------------------
+ */
+#ifdef CONFIG_FUNCTION_TRACER
+
+static void ftrace_function_vearly_trace_call(unsigned long ip,
+       unsigned long parent_ip, struct ftrace_ops *op, struct pt_regs *regs);
+
+#define FTRACE_VEARLY_MAX_ENTRIES 5000
+#define ftrace_vearly_disable() \
+	ftrace_vearly_trace_function = ftrace_stub
+#define ftrace_vearly_enable() \
+	ftrace_vearly_trace_function = ftrace_function_vearly_trace_call
+#define is_ftrace_vearly_enabled() \
+	ftrace_vearly_trace_function != ftrace_stub
+
+/* 
+ * In very early stage : 
+ *  - only CPU0 is running, there is no need to store it
+ *  - tsc counter is used when enabled
+*/
+struct ftrace_vearly_entry {
+	unsigned long ip;
+	unsigned long parent_ip;
+	unsigned long long clock;
+	unsigned long long cycles;
+};
+
+static int ftrace_vearly_enabled __initdata = 0;
+static unsigned long vearly_entries_count __initdata = 0;
+static struct ftrace_vearly_entry 
+	ftrace_vearly_entries[FTRACE_VEARLY_MAX_ENTRIES] __initdata;
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+
+#define VEARLY_FILTER_SIZE 10000
+static unsigned long vearly_notrace_count __initdata = 0;
+static unsigned long ftrace_vearly_notrace_list[VEARLY_FILTER_SIZE] __initdata;
+static unsigned long vearly_filter_count __initdata = 0;
+static unsigned long ftrace_vearly_filter_list[VEARLY_FILTER_SIZE] __initdata;
+static char ftrace_vearly_notrace_buf[COMMAND_LINE_SIZE] __initdata;
+static char ftrace_vearly_filter_buf[COMMAND_LINE_SIZE] __initdata;
+
+static __init int ftrace_vearly_notrace_addr(unsigned long addr)
+{	
+	int i;
+
+	for (i = 0; i < vearly_notrace_count; i++) {
+		if (ftrace_vearly_notrace_list[i] == addr)
+			return 1;
+	}
+	return 0;
+}
+
+static __init int ftrace_vearly_filter_addr(unsigned long addr)
+{	
+	int i;
+
+	for (i = 0; i < vearly_filter_count; i++) {
+		if (ftrace_vearly_filter_list[i] == addr)
+			return 1;
+	}
+	return 0;
+}
+
+static int
+ftrace_vearly_match_record(unsigned long ip, struct ftrace_glob *func_g)
+{
+	char str[KSYM_SYMBOL_LEN];
+	char *modname;
+
+	kallsyms_lookup(ip, NULL, NULL, &modname, str);
+
+	return ftrace_match(str, func_g);
+}
+
+static __init void ftrace_vearly_regex(char *func, int len, int enable)
+{
+	extern unsigned long __start_mcount_loc[];
+	extern unsigned long __stop_mcount_loc[];
+
+	unsigned long *start = __start_mcount_loc;
+	unsigned long *end = __stop_mcount_loc;
+	unsigned long count;
+	unsigned long addr;
+	unsigned long *p;
+	struct ftrace_glob func_g = { .type = MATCH_FULL };
+	int clear_filter = 0;
+
+	count = end - start;
+
+	if (!count)
+		return;
+
+	if (func) {
+		func_g.type = filter_parse_regex(func, len, &func_g.search,
+						 &clear_filter);
+		func_g.len = strlen(func_g.search);
+	}
+
+	p = start;
+	while (p < end) 
+	{
+		addr = ftrace_call_adjust(*p++);
+		if (!addr)
+			continue;
+
+		if (!enable && vearly_notrace_count > VEARLY_FILTER_SIZE)
+			return;
+
+		if (enable && vearly_filter_count > VEARLY_FILTER_SIZE)
+			return;
+
+		if (ftrace_vearly_match_record(addr, &func_g)) 
+		{
+			if (enable){
+				if (!ftrace_vearly_filter_addr(addr))
+					ftrace_vearly_filter_list[vearly_filter_count++] = addr;
+			}else{
+				if (!ftrace_vearly_notrace_addr(addr))
+					ftrace_vearly_notrace_list[vearly_notrace_count++] = addr;
+			}
+		}
+	}
+}
+
+void __init ftrace_set_vearly_filter(char *buf, int enable)
+{
+	char *func;
+
+	while (buf) {
+		func = strsep(&buf, ",");
+		ftrace_vearly_regex(func, strlen(func), enable);
+	}
+}
+
+static __init int set_ftrace_vearly_notrace(char *str)
+{
+	strlcpy(ftrace_vearly_notrace_buf, str, COMMAND_LINE_SIZE);
+	ftrace_set_vearly_filter(ftrace_vearly_notrace_buf, 0);
+    return 1;
+}
+
+static __init int set_ftrace_vearly_filter(char *str)
+{
+	strlcpy(ftrace_vearly_filter_buf, str, COMMAND_LINE_SIZE);
+	ftrace_set_vearly_filter(ftrace_vearly_filter_buf, 1);
+    return 1;
+}
+
+#endif /* CONFIG_DYNAMIC_FTRACE */
+
+static __init int set_ftrace_vearly_enable(char *str)
+{
+	ftrace_vearly_enabled = 1;
+    return 1;
+}
+
+static __init void 
+ftrace_function_vearly_trace_call(unsigned long ip, unsigned long parent_ip,
+			struct ftrace_ops *op, struct pt_regs *regs)
+{
+	struct ftrace_vearly_entry *entry;
+#ifdef CONFIG_DYNAMIC_FTRACE
+	if (vearly_notrace_count && ftrace_vearly_notrace_addr(ip))
+		return;
+
+	if (vearly_filter_count && !ftrace_vearly_filter_addr(ip))
+		return;
+#endif
+
+	if (vearly_entries_count >= FTRACE_VEARLY_MAX_ENTRIES) {
+		ftrace_vearly_disable();
+		return;
+	}
+
+	entry = &ftrace_vearly_entries[vearly_entries_count++];
+
+	entry->ip = ip;
+	entry->parent_ip = parent_ip;
+	entry->clock = trace_clock_local();
+#ifdef CONFIG_X86_TSC
+	entry->cycles = rdtsc();
+#endif
+}
+
+/* 
+ * ns = cycles * (10^6 / cpu_khz)
+ * formula taken from arch/x86/kernel/tsc.c
+ */
+static u64 __init cycles_to_ns(u64 cycles, u64 khz)
+{
+	u64 ns = cycles;
+	u64 dividend, divisor = 0;	
+	const u64 constant = 1000000;
+	
+	if (khz > constant) {
+		// ns /= (khz / constant);
+		dividend = khz;
+		divisor = constant;
+		do_div(dividend, divisor);
+		do_div(ns, dividend);
+	} else {
+		dividend = constant;
+		divisor = khz;
+		do_div(dividend, divisor);
+		ns *= dividend;
+	}
+	return ns;
+}
+void __init ftrace_early_fill_ringbuffer(void *data)
+{
+	int i;
+	u64 ns, delta, prev_ns = 0;
+	unsigned long current_cpu_khz;
+	struct ftrace_vearly_entry *entry;
+	struct trace_array *tr = data;
+
+	if (!ftrace_vearly_enabled)
+		return;
+
+	preempt_disable_notrace();
+	/* retreiving cpu_khz at early stage does't work  */
+	current_cpu_khz = native_calibrate_cpu();
+	for (i = 0; i < vearly_entries_count; i++) 
+	{
+		entry = &ftrace_vearly_entries[i];
+
+		if (entry->cycles && current_cpu_khz) 
+			ns = cycles_to_ns(entry->cycles, current_cpu_khz);
+		else
+			ns = entry->clock;
+		
+		delta = ns - prev_ns;
+		prev_ns = ns;
+		trace_function_timestamp(tr, entry->ip, entry->parent_ip, 0, 0, delta);
+	}
+	preempt_enable_notrace();
+}
+/* 
+ * this will be used as __setup_param, because there is no 
+ * very_early_param(str, fn)
+*/
+static struct obs_kernel_param ftrace_vearly_params[] __initdata = 
+{
+	{ .str = "ftrace_vearly", .setup_func = set_ftrace_vearly_enable },
+#ifdef CONFIG_DYNAMIC_FTRACE
+	{ .str = "ftrace_notrace", .setup_func = set_ftrace_vearly_notrace },
+	{ .str = "ftrace_filter", .setup_func = set_ftrace_vearly_filter },
+#endif
+};
+
+static int __init ftrace_do_very_early_param(char *param, char *val,
+				 const char *unused, void *arg)
+{
+	int i;
+	struct obs_kernel_param *p;
+	int size = sizeof(ftrace_vearly_params)/sizeof(struct obs_kernel_param);
+
+	for (i = 0; i < size; i++) {
+		p = &ftrace_vearly_params[i];
+		if (strcmp(param, p->str) == 0) {
+			p->setup_func(val);
+			return 0;
+		}
+	}
+	return 0;
+}
+
+void __init ftrace_early_init(char *command_line)
+{
+	static char tmp_cmdline[COMMAND_LINE_SIZE] __initdata;
+
+	strlcpy(tmp_cmdline, command_line, COMMAND_LINE_SIZE);
+	parse_args("ftrace very early options", tmp_cmdline, NULL, 0, 0, 0, NULL,
+		ftrace_do_very_early_param);
+
+	if (ftrace_vearly_enabled)
+		ftrace_vearly_enable();
+}
+
+void __init ftrace_early_shutdown(void)
+{
+	if (!ftrace_vearly_enabled)
+		return;
+	
+	ftrace_vearly_disable();
+		
+#ifdef CONFIG_DYNAMIC_FTRACE
+	pr_info("ftrace: vearly %lu entries, ftrace_notrace=%lu, ftrace_filter=%lu",
+	vearly_entries_count, vearly_notrace_count, vearly_filter_count);
+#else
+	pr_info("ftrace: vearly %lu recorded entries", vearly_entries_count);
+#endif
+}
+
+#else /* !CONFIG_FUNCTION_TRACER */
+
+inline void __init ftrace_early_init(char *command_line) { }
+
+inline void __init ftrace_early_shutdown(void) { }
+
+#endif /* CONFIG_FUNCTION_TRACER */
\ No newline at end of file
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 752e5daf0896..d64b3c176f95 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -2554,6 +2554,33 @@ trace_function(struct trace_array *tr,
 	}
 }
 
+void
+trace_function_timestamp(struct trace_array *tr,
+	       unsigned long ip, unsigned long parent_ip, unsigned long flags,
+	       int pc, u64 timestamp)
+{
+	struct trace_event_call *call = &event_function;
+	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer_event *event;
+	struct ftrace_entry *entry;
+
+	event = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry), 
+						flags, pc);
+	if (!event)
+		return;
+
+	event->time_delta = timestamp;
+
+	entry = ring_buffer_event_data(event);
+	entry->ip = ip;
+	entry->parent_ip = parent_ip;
+
+	if (!call_filter_check_discard(call, entry, buffer, event)) {
+		if (static_branch_unlikely(&ftrace_exports_enabled))
+			ftrace_exports(event);
+		__buffer_unlock_commit(buffer, event);
+	}
+}
 #ifdef CONFIG_STACKTRACE
 
 #define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))
@@ -8276,6 +8303,7 @@ void ftrace_dump(enum ftrace_dump_mode oops_dump_mode)
 }
 EXPORT_SYMBOL_GPL(ftrace_dump);
 
+
 __init static int tracer_alloc_buffers(void)
 {
 	int ring_buf_size;
@@ -8364,6 +8392,9 @@ __init static int tracer_alloc_buffers(void)
 	/* Function tracing may start here (via kernel command line) */
 	init_function_trace();
 
+	/* Fill ringbuffer with very early events */
+	ftrace_early_fill_ringbuffer(&global_trace);
+
 	/* All seems OK, enable tracing */
 	tracing_disabled = 0;
 
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 652c682707cd..9aa22e261812 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -624,6 +624,10 @@ void trace_init_global_iter(struct trace_iterator *iter);
 
 void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
+void trace_function_timestamp(struct trace_array *tr,
+		    unsigned long ip,
+		    unsigned long parent_ip,
+		    unsigned long flags, int pc, u64 timestamp);
 void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-- 
2.11.0

